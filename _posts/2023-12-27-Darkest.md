---
layout: post
title: "Dark Launching and Shadow Testing"
description: "'[Dark Launching](https://martinfowler.com/bliki/DarkLaunching.html)' refers to feature enablement in production behind the scenes, like 'Shadow Testing', which is the testing process of new application features in the background without making them generally available."
date: 2023-12-27 08:41:50
author: Erhan Bagdemir
comments: true
keywords: "Testing, Dark Launching, Shadow Testing, Release"
category: Testing
image:  '/images/21.jpg'
tags:
- Testing
- Dark Launching
- Shadow Testing
---

'[Dark Launching](https://martinfowler.com/bliki/DarkLaunching.html)' refers to feature enablement in production behind the scenes, like '[Shadow Testing](https://microsoft.github.io/code-with-engineering-playbook/automated-testing/shadow-testing/)', which is the testing process of new application features in the background without making them generally available. It aims to activate new software features before the GA without interrupting the operations and being transparent to customers. During the testing phase, you are to monitor, for example, whether the new integration works seamlessly with existing components under realistic data and load. '[Dark Launching](https://martinfowler.com/bliki/DarkLaunching.html)' and '[Shadow Testing](https://microsoft.github.io/code-with-engineering-playbook/automated-testing/shadow-testing/)' are indeed very similar in practice and differ only in their intended purpose â€” and occasionally, they are used interchangeably. 

The main goal in '[Dark Launching](https://martinfowler.com/bliki/DarkLaunching.html)' is to observe the behavior of the new application features under the realistic workload in the production environment. Clients are mostly unaware of the test running in the background. '[Shadow Testing](https://microsoft.github.io/code-with-engineering-playbook/automated-testing/shadow-testing/)', very like '[Dark Launching](https://martinfowler.com/bliki/DarkLaunching.html)', is usually run on production systems against real customer requests, which are processed by the existing software components during [Shadow Testing](https://microsoft.github.io/code-with-engineering-playbook/automated-testing/shadow-testing/), while triggering the new components at the same time. The response from the new component is often not returned to the customer, and it is to simulate the integration of the new feature with the rest of the application. In case of an error, the output is logged out in a way that the engineering team can investigate, and the response from the existing integration is returned to the customer.

'Shadow Testing' can be considered as an integration test using real data without the customer's awareness. You might wonder why integration tests alone aren't sufficient for testing new software components. Integration tests typically involve using test users and synthetic data to mimic production data. However, synthetic data may not capture the full range of variability present in real customer data within production systems. Consequently, this can lead to gaps in test coverage.

But what about canary deployment and monitoring? It works, but if you need to take samples from the new integration and compare the results with the old one over an extended period, the canary might not be the right option. This would require the deployment pipeline to be put on hold and might not be a real option as it would delay releases. 

Shadow testing can be handy, for example, while switching between different versions of the same endpoint. Indeed, the backend teams, which I was the member of in the past, created such proxies between the connector layer and business logic to compare the results of two endpoints before moving to a new version of the integration. This approach assures the compatibility and reduces repeated roll-out-and-backs.

Testing the software features in production might sound like out of the left field, but if you find a way to sandbox the testing safely, I think, it is just like live probing the new feature. "[Darkest]([https://github.com/reevik/darkest](https://github.com/reevik/darkest))" implements this sandbox, a project which I set out for dark launching, is a simple framework, to validate two execution paths, they are implemented as commands. It will route the incoming requests depending on the routing criterion and assesses the outcome from the execution of both command. You can enhance the library by implementing new Monitorables, for example, whenever the new endpoint gets called or the endpoint responses diverge, you can register new metrics at your monitoring endpoint and make the roll-out visible. That's it. If you have new ideas, feel free to join the [project]([https://github.com/reevik/darkest](https://github.com/reevik/darkest)).
